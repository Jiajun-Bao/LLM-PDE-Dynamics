{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db12cb3b",
   "metadata": {},
   "source": [
    "This notebook prepares the data for the subsequent notebook `15-Step-Analyze.ipynb`, which generates the figure illustrating multi-step prediction performance against naive temporal baselines, as described in Supplementary Material Section 8.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89f6cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import torch\n",
    "num_devices = torch.cuda.device_count()\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from tqdm import tqdm\n",
    "from data_processing import (\n",
    "    SimpleSerializerSettings, scale_2d_array, unscale_2d_array,\n",
    "    serialize_2d_integers, deserialize_2d_integers\n",
    ")\n",
    "\n",
    "from allen_cahn_equation import (\n",
    "    compute_exact_solution_random_ic_vary_Nx,\n",
    "    visualize_spline_ic,\n",
    ")\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def train_ar_model(data_quantized, order):\n",
    "    n_timesteps, n_spatial = data_quantized.shape\n",
    "    X = []\n",
    "    y = []\n",
    "    for x_idx in range(n_spatial):\n",
    "        time_series = data_quantized[:, x_idx]\n",
    "        for t in range(order, n_timesteps):\n",
    "            X.append(time_series[t-order:t][::-1])\n",
    "            y.append(time_series[t])\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def ar_multi_step_prediction(model, initial_data_quantized, order, n_steps):\n",
    "    n_spatial = initial_data_quantized.shape[1]\n",
    "    predictions = np.zeros((n_steps, n_spatial), dtype=int)\n",
    "    buffer = initial_data_quantized[-order:].copy().astype(float)\n",
    "    for step in range(n_steps):\n",
    "        current_pred = np.zeros(n_spatial, dtype=int)\n",
    "        for x_idx in range(n_spatial):\n",
    "            features = buffer[:, x_idx][::-1].reshape(1, -1)\n",
    "            pred_value = model.predict(features)[0]\n",
    "            pred_value = int(round(pred_value))\n",
    "            current_pred[x_idx] = pred_value\n",
    "        predictions[step] = current_pred\n",
    "        buffer = np.vstack([buffer[1:], current_pred])\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "\n",
    "def temporal_repeat_prediction(initial_data_quantized, n_steps):\n",
    "    last_timestep = initial_data_quantized[-1]\n",
    "    predictions = np.tile(last_timestep, (n_steps, 1))\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def last_token_repeat_prediction(initial_data_quantized, n_steps):\n",
    "    n_spatial = initial_data_quantized.shape[1]\n",
    "    last_token_value = initial_data_quantized[-1, -1]\n",
    "    predictions = np.full((n_steps, n_spatial), last_token_value, dtype=int)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def ar_predictions_evaluate(full_serialized_data, input_time_steps,\n",
    "                                           number_of_future_predictions, ar_order,\n",
    "                                           Nx, settings, vmin, vmax):\n",
    "    u_quantized = deserialize_2d_integers(full_serialized_data, settings)\n",
    "    \n",
    "    u_training_quantized = u_quantized[:input_time_steps]\n",
    "    u_ground_truth_quantized = u_quantized[input_time_steps:input_time_steps + number_of_future_predictions]\n",
    "    model = train_ar_model(u_training_quantized, ar_order)\n",
    "    predictions_quantized = ar_multi_step_prediction(\n",
    "        model, u_training_quantized, ar_order, number_of_future_predictions\n",
    "    )\n",
    "    predictions_unscaled = unscale_2d_array(predictions_quantized, vmin, vmax)\n",
    "    ground_truth_unscaled = unscale_2d_array(u_ground_truth_quantized, vmin, vmax)\n",
    "    max_diffs = []\n",
    "    rmses = []\n",
    "    \n",
    "    for step in range(number_of_future_predictions):\n",
    "        pred_step = predictions_unscaled[step]\n",
    "        true_step = ground_truth_unscaled[step]\n",
    "        max_diff = np.max(np.abs(pred_step - true_step))\n",
    "        rmse = np.sqrt(np.mean((pred_step - true_step)**2))\n",
    "        max_diffs.append(max_diff)\n",
    "        rmses.append(rmse)\n",
    "    \n",
    "    return np.array(max_diffs), np.array(rmses)\n",
    "\n",
    "\n",
    "def temporal_repeat_predictions_evaluate(full_serialized_data, input_time_steps,\n",
    "                                                  number_of_future_predictions,\n",
    "                                                  Nx, settings, vmin, vmax):\n",
    "    u_quantized = deserialize_2d_integers(full_serialized_data, settings)\n",
    "    u_training_quantized = u_quantized[:input_time_steps]\n",
    "    u_ground_truth_quantized = u_quantized[input_time_steps:input_time_steps + number_of_future_predictions]\n",
    "    predictions_quantized = temporal_repeat_prediction(u_training_quantized, number_of_future_predictions)\n",
    "    predictions_unscaled = unscale_2d_array(predictions_quantized, vmin, vmax)\n",
    "    ground_truth_unscaled = unscale_2d_array(u_ground_truth_quantized, vmin, vmax)\n",
    "    \n",
    "    max_diffs = []\n",
    "    rmses = []\n",
    "    \n",
    "    for step in range(number_of_future_predictions):\n",
    "        pred_step = predictions_unscaled[step]\n",
    "        true_step = ground_truth_unscaled[step]\n",
    "        \n",
    "        max_diff = np.max(np.abs(pred_step - true_step))\n",
    "        rmse = np.sqrt(np.mean((pred_step - true_step)**2))\n",
    "        \n",
    "        max_diffs.append(max_diff)\n",
    "        rmses.append(rmse)\n",
    "    \n",
    "    return np.array(max_diffs), np.array(rmses)\n",
    "\n",
    "\n",
    "def last_token_repeat_predictions_evaluate(full_serialized_data, input_time_steps,\n",
    "                                                    number_of_future_predictions,\n",
    "                                                    Nx, settings, vmin, vmax):\n",
    "    u_quantized = deserialize_2d_integers(full_serialized_data, settings)\n",
    "    \n",
    "    u_training_quantized = u_quantized[:input_time_steps]\n",
    "    u_ground_truth_quantized = u_quantized[input_time_steps:input_time_steps + number_of_future_predictions]\n",
    "    \n",
    "    predictions_quantized = last_token_repeat_prediction(u_training_quantized, number_of_future_predictions)\n",
    "    \n",
    "    predictions_unscaled = unscale_2d_array(predictions_quantized, vmin, vmax)\n",
    "    ground_truth_unscaled = unscale_2d_array(u_ground_truth_quantized, vmin, vmax)\n",
    "    \n",
    "    max_diffs = []\n",
    "    rmses = []\n",
    "    \n",
    "    for step in range(number_of_future_predictions):\n",
    "        pred_step = predictions_unscaled[step]\n",
    "        true_step = ground_truth_unscaled[step]\n",
    "        \n",
    "        max_diff = np.max(np.abs(pred_step - true_step))\n",
    "        rmse = np.sqrt(np.mean((pred_step - true_step)**2))\n",
    "        \n",
    "        max_diffs.append(max_diff)\n",
    "        rmses.append(rmse)\n",
    "    \n",
    "    return np.array(max_diffs), np.array(rmses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd59653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters for the Allen-Cahn equation\n",
    "L = 2       # Length of the spatial domain\n",
    "k = 0.001   # Thermal diffusivity\n",
    "T = 1.0     # Total simulation time\n",
    "Nt = 50     # Number of time steps\n",
    "dt = T/Nt\n",
    "Nx = 14     # Number of spatial steps (excluding boundary points)\n",
    "dx = L/(Nx+1)\n",
    "settings = SimpleSerializerSettings(space_sep=\",\", time_sep=\";\")\n",
    "input_time_steps = 31\n",
    "number_of_future_predictions = 15\n",
    "n_ics = 20\n",
    "\n",
    "# Generate all random initial conditions and spline objects\n",
    "stored_initial_conditions = []\n",
    "stored_spline_objects = []\n",
    "for ic_seed in range(n_ics):\n",
    "    random.seed(ic_seed)\n",
    "    np.random.seed(ic_seed)\n",
    "    torch.manual_seed(ic_seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(ic_seed)\n",
    "    init_cond_random = np.random.uniform(-0.5, 0.5, size=Nx)\n",
    "    stored_initial_conditions.append(init_cond_random.copy())\n",
    "    fig, cs = visualize_spline_ic(L, Nx, init_cond_random)\n",
    "    plt.close(fig)\n",
    "    stored_spline_objects.append(cs)\n",
    "\n",
    "stored_initial_conditions_array = np.array(stored_initial_conditions)\n",
    "\n",
    "# AR(1) model\n",
    "all_ar_results = {'max_diffs': [], 'rmses': []}\n",
    "for ic_seed in tqdm(range(n_ics), desc=\"AR(1)\"):\n",
    "    # Use the stored initial condition and spline\n",
    "    init_cond_random = stored_initial_conditions[ic_seed]\n",
    "    cs = stored_spline_objects[ic_seed]\n",
    "    # Compute exact solution for this initial condition\n",
    "    u_exact = compute_exact_solution_random_ic_vary_Nx(L, k, T, Nx, Nt, spline_obj=cs)\n",
    "    u_exact_scaled, vmin_exact, vmax_exact = scale_2d_array(u_exact)\n",
    "    u_exact_serialized = serialize_2d_integers(u_exact_scaled, settings)\n",
    "    max_diffs, rmses = ar_predictions_evaluate(\n",
    "        full_serialized_data=u_exact_serialized,\n",
    "        input_time_steps=input_time_steps,\n",
    "        number_of_future_predictions=number_of_future_predictions,\n",
    "        ar_order=1,\n",
    "        Nx=Nx,\n",
    "        settings=settings,\n",
    "        vmin=vmin_exact,\n",
    "        vmax=vmax_exact\n",
    "    )\n",
    "    \n",
    "    all_ar_results['max_diffs'].append(max_diffs)\n",
    "    all_ar_results['rmses'].append(rmses)\n",
    "\n",
    "# Temporal Repeat baseline\n",
    "all_temporal_repeat_results = {'max_diffs': [], 'rmses': []}\n",
    "for ic_seed in tqdm(range(n_ics), desc=\"Temporal Repeat\"):\n",
    "    # Use the stored initial condition and spline\n",
    "    init_cond_random = stored_initial_conditions[ic_seed]\n",
    "    cs = stored_spline_objects[ic_seed]\n",
    "    # Compute exact solution for this initial condition\n",
    "    u_exact = compute_exact_solution_random_ic_vary_Nx(L, k, T, Nx, Nt, spline_obj=cs)\n",
    "    u_exact_scaled, vmin_exact, vmax_exact = scale_2d_array(u_exact)\n",
    "    u_exact_serialized = serialize_2d_integers(u_exact_scaled, settings)\n",
    "    max_diffs, rmses = temporal_repeat_predictions_evaluate(\n",
    "        full_serialized_data=u_exact_serialized,\n",
    "        input_time_steps=input_time_steps,\n",
    "        number_of_future_predictions=number_of_future_predictions,\n",
    "        Nx=Nx,\n",
    "        settings=settings,\n",
    "        vmin=vmin_exact,\n",
    "        vmax=vmax_exact\n",
    "    )\n",
    "    all_temporal_repeat_results['max_diffs'].append(max_diffs)\n",
    "    all_temporal_repeat_results['rmses'].append(rmses)\n",
    "\n",
    "# Last-Token Repeat baseline\n",
    "all_last_token_repeat_results = {'max_diffs': [], 'rmses': []}\n",
    "\n",
    "for ic_seed in tqdm(range(n_ics), desc=\"Last-Token Repeat\"):\n",
    "    # Use the stored initial condition and spline\n",
    "    init_cond_random = stored_initial_conditions[ic_seed]\n",
    "    cs = stored_spline_objects[ic_seed]\n",
    "    # Compute exact solution for this initial condition\n",
    "    u_exact = compute_exact_solution_random_ic_vary_Nx(L, k, T, Nx, Nt, spline_obj=cs)\n",
    "    u_exact_scaled, vmin_exact, vmax_exact = scale_2d_array(u_exact)\n",
    "    u_exact_serialized = serialize_2d_integers(u_exact_scaled, settings)\n",
    "    # Run last-token repeat baseline for this initial condition\n",
    "    max_diffs, rmses = last_token_repeat_predictions_evaluate(\n",
    "        full_serialized_data=u_exact_serialized,\n",
    "        input_time_steps=input_time_steps,\n",
    "        number_of_future_predictions=number_of_future_predictions,\n",
    "        Nx=Nx,\n",
    "        settings=settings,\n",
    "        vmin=vmin_exact,\n",
    "        vmax=vmax_exact\n",
    "    )\n",
    "    \n",
    "    all_last_token_repeat_results['max_diffs'].append(max_diffs)\n",
    "    all_last_token_repeat_results['rmses'].append(rmses)\n",
    "\n",
    "\n",
    "def log_ci(mean, std, n, tcrit):\n",
    "    se = std / np.sqrt(n)\n",
    "    se_log = se / (mean * np.log(10))\n",
    "    mean_log = np.log10(mean)\n",
    "    delta_log = tcrit * se_log\n",
    "    return 10**(mean_log - delta_log), 10**(mean_log + delta_log)\n",
    "\n",
    "t_critical = stats.t.ppf(0.975, n_ics - 1)\n",
    "\n",
    "averaged_results = {}\n",
    "\n",
    "all_max_diffs = np.array(all_ar_results['max_diffs'])\n",
    "all_rmses = np.array(all_ar_results['rmses'])\n",
    "\n",
    "avg_max_diffs = np.mean(all_max_diffs, axis=0)\n",
    "avg_rmses = np.mean(all_rmses, axis=0)\n",
    "std_max_diffs = np.std(all_max_diffs, axis=0, ddof=1)\n",
    "std_rmses = np.std(all_rmses, axis=0, ddof=1)\n",
    "\n",
    "ci_lower_max_diffs = []\n",
    "ci_upper_max_diffs = []\n",
    "ci_lower_rmses = []\n",
    "ci_upper_rmses = []\n",
    "\n",
    "for mean, std in zip(avg_max_diffs, std_max_diffs):\n",
    "    lower, upper = log_ci(mean, std, n_ics, t_critical)\n",
    "    ci_lower_max_diffs.append(lower)\n",
    "    ci_upper_max_diffs.append(upper)\n",
    "\n",
    "for mean, std in zip(avg_rmses, std_rmses):\n",
    "    lower, upper = log_ci(mean, std, n_ics, t_critical)\n",
    "    ci_lower_rmses.append(lower)\n",
    "    ci_upper_rmses.append(upper)\n",
    "\n",
    "averaged_results['AR1'] = {\n",
    "    'max_diffs': avg_max_diffs.tolist(),\n",
    "    'rmses': avg_rmses.tolist(),\n",
    "    'std_max_diffs': std_max_diffs.tolist(),\n",
    "    'std_rmses': std_rmses.tolist(),\n",
    "    'ci_lower_max_diffs': ci_lower_max_diffs,\n",
    "    'ci_upper_max_diffs': ci_upper_max_diffs,\n",
    "    'ci_lower_rmses': ci_lower_rmses,\n",
    "    'ci_upper_rmses': ci_upper_rmses,\n",
    "}\n",
    "\n",
    "all_max_diffs = np.array(all_temporal_repeat_results['max_diffs'])\n",
    "all_rmses = np.array(all_temporal_repeat_results['rmses'])\n",
    "\n",
    "avg_max_diffs = np.mean(all_max_diffs, axis=0)\n",
    "avg_rmses = np.mean(all_rmses, axis=0)\n",
    "std_max_diffs = np.std(all_max_diffs, axis=0, ddof=1)\n",
    "std_rmses = np.std(all_rmses, axis=0, ddof=1)\n",
    "\n",
    "ci_lower_max_diffs = []\n",
    "ci_upper_max_diffs = []\n",
    "ci_lower_rmses = []\n",
    "ci_upper_rmses = []\n",
    "\n",
    "for mean, std in zip(avg_max_diffs, std_max_diffs):\n",
    "    lower, upper = log_ci(mean, std, n_ics, t_critical)\n",
    "    ci_lower_max_diffs.append(lower)\n",
    "    ci_upper_max_diffs.append(upper)\n",
    "\n",
    "for mean, std in zip(avg_rmses, std_rmses):\n",
    "    lower, upper = log_ci(mean, std, n_ics, t_critical)\n",
    "    ci_lower_rmses.append(lower)\n",
    "    ci_upper_rmses.append(upper)\n",
    "\n",
    "averaged_results['Temporal_Repeat'] = {\n",
    "    'max_diffs': avg_max_diffs.tolist(),\n",
    "    'rmses': avg_rmses.tolist(),\n",
    "    'std_max_diffs': std_max_diffs.tolist(),\n",
    "    'std_rmses': std_rmses.tolist(),\n",
    "    'ci_lower_max_diffs': ci_lower_max_diffs,\n",
    "    'ci_upper_max_diffs': ci_upper_max_diffs,\n",
    "    'ci_lower_rmses': ci_lower_rmses,\n",
    "    'ci_upper_rmses': ci_upper_rmses,\n",
    "}\n",
    "\n",
    "all_max_diffs = np.array(all_last_token_repeat_results['max_diffs'])\n",
    "all_rmses = np.array(all_last_token_repeat_results['rmses'])\n",
    "\n",
    "avg_max_diffs = np.mean(all_max_diffs, axis=0)\n",
    "avg_rmses = np.mean(all_rmses, axis=0)\n",
    "std_max_diffs = np.std(all_max_diffs, axis=0, ddof=1)\n",
    "std_rmses = np.std(all_rmses, axis=0, ddof=1)\n",
    "\n",
    "ci_lower_max_diffs = []\n",
    "ci_upper_max_diffs = []\n",
    "ci_lower_rmses = []\n",
    "ci_upper_rmses = []\n",
    "\n",
    "for mean, std in zip(avg_max_diffs, std_max_diffs):\n",
    "    lower, upper = log_ci(mean, std, n_ics, t_critical)\n",
    "    ci_lower_max_diffs.append(lower)\n",
    "    ci_upper_max_diffs.append(upper)\n",
    "\n",
    "for mean, std in zip(avg_rmses, std_rmses):\n",
    "    lower, upper = log_ci(mean, std, n_ics, t_critical)\n",
    "    ci_lower_rmses.append(lower)\n",
    "    ci_upper_rmses.append(upper)\n",
    "\n",
    "averaged_results['Last_Token_Repeat'] = {\n",
    "    'max_diffs': avg_max_diffs.tolist(),\n",
    "    'rmses': avg_rmses.tolist(),\n",
    "    'std_max_diffs': std_max_diffs.tolist(),\n",
    "    'std_rmses': std_rmses.tolist(),\n",
    "    'ci_lower_max_diffs': ci_lower_max_diffs,\n",
    "    'ci_upper_max_diffs': ci_upper_max_diffs,\n",
    "    'ci_lower_rmses': ci_lower_rmses,\n",
    "    'ci_upper_rmses': ci_upper_rmses,\n",
    "}\n",
    "\n",
    "all_baseline_max_errors_per_step = []\n",
    "all_baseline_rmse_errors_per_step = []\n",
    "\n",
    "for ic_seed in range(n_ics):\n",
    "    init_cond_random = stored_initial_conditions[ic_seed]\n",
    "    cs = stored_spline_objects[ic_seed]\n",
    "    u_exact = compute_exact_solution_random_ic_vary_Nx(L, k, T, Nx, Nt, spline_obj=cs)\n",
    "    \n",
    "    u_exact_scaled, vmin_exact, vmax_exact = scale_2d_array(u_exact)\n",
    "    u_exact_serialized = serialize_2d_integers(u_exact_scaled, settings)\n",
    "    u_exact_parsed = deserialize_2d_integers(u_exact_serialized, settings)\n",
    "    u_exact_unscaled = unscale_2d_array(u_exact_parsed, vmin_exact, vmax_exact)\n",
    "    \n",
    "    seed_max_errors_per_step = []\n",
    "    seed_rmse_errors_per_step = []\n",
    "    \n",
    "    for t in range(u_exact.shape[0]):\n",
    "        max_err_t = np.max(np.abs(u_exact[t] - u_exact_unscaled[t]))\n",
    "        rmse_err_t = np.sqrt(np.mean((u_exact[t] - u_exact_unscaled[t])**2))\n",
    "        seed_max_errors_per_step.append(max_err_t)\n",
    "        seed_rmse_errors_per_step.append(rmse_err_t)\n",
    "    \n",
    "    all_baseline_max_errors_per_step.append(seed_max_errors_per_step)\n",
    "    all_baseline_rmse_errors_per_step.append(seed_rmse_errors_per_step)\n",
    "\n",
    "all_baseline_max_errors_per_step = np.array(all_baseline_max_errors_per_step)\n",
    "all_baseline_rmse_errors_per_step = np.array(all_baseline_rmse_errors_per_step)\n",
    "\n",
    "avg_baseline_max_errors_per_step = np.mean(all_baseline_max_errors_per_step, axis=0)\n",
    "avg_baseline_rmse_errors_per_step = np.mean(all_baseline_rmse_errors_per_step, axis=0)\n",
    "\n",
    "avg_baseline_max_errors_prediction = avg_baseline_max_errors_per_step[input_time_steps:]\n",
    "avg_baseline_rmse_errors_prediction = avg_baseline_rmse_errors_per_step[input_time_steps:]\n",
    "\n",
    "np.savez_compressed(\n",
    "    \"AR_Repeat_15_step.npz\",\n",
    "    ar_results=averaged_results,\n",
    "    all_ar_results=all_ar_results,\n",
    "    all_temporal_repeat_results=all_temporal_repeat_results,\n",
    "    all_last_token_repeat_results=all_last_token_repeat_results,\n",
    "    avg_baseline_max_errors_prediction=avg_baseline_max_errors_prediction,\n",
    "    avg_baseline_rmse_errors_prediction=avg_baseline_rmse_errors_prediction,\n",
    "    stored_initial_conditions=stored_initial_conditions_array,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smollm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
